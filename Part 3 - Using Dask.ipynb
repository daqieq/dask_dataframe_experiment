{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask DataFrame Experiment\n",
    "\n",
    "## Part 3\n",
    "\n",
    "Use Dask to convert a pandas dataframe into a text file in 'fixed position' format, with custom encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "\n",
    "pp = PrettyPrinter(indent=4).pprint\n",
    "# %config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom data encoding helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBCDIC:\n",
    "    \n",
    "    def __init__(self, logger, cp='cp037'):\n",
    "        self.logger = logger\n",
    "        self.cp = cp\n",
    "        self.cp_blank = ' '.encode(cp)\n",
    "    \n",
    "    def _log_error_msg1(msg: str):\n",
    "        self.logger(f\"ERROR: Function ::{msg}:: Returned error.\")\n",
    "        \n",
    "    def _log_error_msg2(msg: tuple):\n",
    "        self.logger(f\"ERROR: Function ::{msg[0]}:: {msg[1]}.\")\n",
    "    \n",
    "    def to_char(self, string, size) -> bytes:\n",
    "        try:\n",
    "            s = string.encode(self.cp)[:size]\n",
    "            if len(s) < size:\n",
    "                s += self.cp_blank * (size - len(s))\n",
    "            return s\n",
    "        except:\n",
    "            self._log_error_msg1('to_char()')\n",
    "\n",
    "    def to_pib(self, integer: int, size: int) -> bytes:\n",
    "        if size not in [1, 2, 4, 8]:\n",
    "            self._log_error_msg2(('to_pib()', \"'size' must be one of: 1, 2, 4, or 8.\"))\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            h = hex(integer)[2:]\n",
    "            if len(h) % 2 == 1:\n",
    "                h = '0' + h\n",
    "                \n",
    "            s = int(len(h) / 2)\n",
    "            if s > size:\n",
    "                self._log_error_msg2(('to_pib()', \"'size' not large enough for the 'integer'.\"))\n",
    "                return None\n",
    "            elif s < size:\n",
    "                h = '0' * ((size - s) * 2) + h\n",
    "            \n",
    "            return bytes.fromhex(h)\n",
    "        except:\n",
    "            self._log_error_msg1('to_pib()')\n",
    "            return None\n",
    "        \n",
    "    # Truncated for brevity\n",
    "    \n",
    "E = EBCDIC(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will speed up access to the data during multiple calls\n",
    "records_d = OrderedDict()\n",
    "\n",
    "# Convert a single 'cell' value with the given conversion type\n",
    "def convert_to_ebcdic(value, size, conv, pre):\n",
    "    \n",
    "    pre_b = b''\n",
    "    \n",
    "    if pre != '':\n",
    "        pre_b = E.to_char(pre, len(pre))\n",
    "\n",
    "    if conv == 'CHAR':\n",
    "        if type(value) != str:\n",
    "            value = str(value)\n",
    "        return pre_b + E.to_char(value, size)\n",
    "    \n",
    "    # Truncated \n",
    "    \n",
    "# Map an entire record (row) to a bytes array\n",
    "def map_rec_to_bytes(rec: pd.Series):\n",
    "    \n",
    "    # Get the record layout dict for this record\n",
    "    record_d = records_d[rec['rec_lbl']]\n",
    "    \n",
    "    # Start the binary record\n",
    "    b_rec = b''\n",
    "    \n",
    "    # Mark the current position in the fixed position record\n",
    "    pos = 1\n",
    "    \n",
    "    # Loop through all fields in the layout\n",
    "    for field in record_d.keys():\n",
    "        start = record_d[field]['start']\n",
    "        length = record_d[field]['length']\n",
    "        conversion = record_d[field]['conversion']\n",
    "        \n",
    "        if pos < start:\n",
    "            prepend = ' ' * (start-pos)\n",
    "        else:\n",
    "            prepend = ''\n",
    "        \n",
    "        pos = start + length\n",
    "        \n",
    "        b_rec += convert_to_ebcdic(rec[field], length, conversion, prepend)\n",
    "    \n",
    "    # Add 4 positions for the IBM RDW - minus 1 because pos tracks the position of the next value\n",
    "    rec_len = pos + 4 - 1\n",
    "    \n",
    "    # The IBM RDW is made up of 2 parts\n",
    "    ibm_rdw = E.to_pib(rec_len, 2) + E.to_pib(0, 2)\n",
    "    \n",
    "    return ibm_rdw + b_rec\n",
    "\n",
    "# Map an entire pandas dataframe with the function\n",
    "def map_pandas_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = df.apply(\n",
    "        map_rec_to_bytes,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    max_length = s.str.len().max()\n",
    "    \n",
    "    return pd.DataFrame(\n",
    "        [[\n",
    "            b''.join(s.tolist()), max_length\n",
    "        ]],\n",
    "        columns=['Binary Data', 'Rec Length']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the data using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data location\n",
    "DATA = 'data'\n",
    "\n",
    "def dask_convert_data(count=5000, nworkers=1, multiple=1):\n",
    "    file_path_data = os.path.join(os.getcwd(), DATA, str(count)+'_data.pickle')\n",
    "    file_path_layout = os.path.join(os.getcwd(), DATA, str(count)+'_layout.pickle')\n",
    "    file_path_records = os.path.join(os.getcwd(), DATA, str(count)+'_records.pickle')\n",
    "\n",
    "    with open(file_path_data, 'rb') as f:\n",
    "        data_df = pickle.load(f)\n",
    "    with open(file_path_layout, 'rb') as f:\n",
    "        layout_df = pickle.load(f)\n",
    "    with open(file_path_records, 'rb') as f:\n",
    "        records_df = pickle.load(f)\n",
    "        \n",
    "    print(data_df.shape)\n",
    "    multiple -= 1\n",
    "    if multiple > 0:\n",
    "        data_df = data_df.append([data_df]*multiple,ignore_index=True)\n",
    "    print(data_df.shape)\n",
    "    \n",
    "    # Convert the layout dataframe to dict\n",
    "    # Iterate over each unique record \n",
    "    for record in layout_df['rec'].unique():\n",
    "        fields_d = OrderedDict()\n",
    "        # Iterate over each unique field 'name' in the record\n",
    "        for field in layout_df[layout_df['rec']==record]['name'].unique():\n",
    "            fields_d[field] = layout_df[\n",
    "                (layout_df['rec'] == record) & (layout_df['name'] == field)\n",
    "            ][['start', 'length', 'conversion']].to_dict('records')[0]\n",
    "        records_d[record] = fields_d\n",
    "    \n",
    "    # Add record label to data dataframe\n",
    "    records_l = records_df.values.tolist()\n",
    "    \n",
    "    data_df['rec_lbl'] = ''\n",
    "    \n",
    "    for rec in records_l:\n",
    "        data_df.loc[eval(rec[1]), ['rec_lbl']] = rec[0]\n",
    "        \n",
    "    parts = nworkers*2\n",
    "    \n",
    "    # Start measuring time\n",
    "    dask_start = time.time()\n",
    "    \n",
    "    # Set up Dask client\n",
    "    client = Client(n_workers=nworkers, threads_per_worker=2)\n",
    "\n",
    "    print(f\"Client using processes: {client.cluster.processes}\")\n",
    "    pp(client.cluster.workers)\n",
    "    \n",
    "    ddf = dd.from_pandas(data_df, npartitions=parts)\n",
    "\n",
    "    # Map the data\n",
    "    result = ddf.map_partitions(\n",
    "        map_pandas_df,\n",
    "        meta={\n",
    "            'Binary Data': 'object',\n",
    "            'Rec Length': 'int64'\n",
    "        }\n",
    "    ).compute()\n",
    "\n",
    "    dask_result = pd.DataFrame(\n",
    "        [[\n",
    "            b''.join(list(result['Binary Data'].values)), result['Rec Length'].max()\n",
    "        ]],\n",
    "        columns=['Binary Data', 'Rec Length']\n",
    "    )\n",
    "    \n",
    "    dask_secs = time.time() - dask_start\n",
    "    print(f\"Total time: {dask_secs:.3f} seconds\")\n",
    "    \n",
    "    return dask_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3501243, 12)\n",
      "(7002486, 12)\n",
      "Client using processes: True\n",
      "{   0: <Nanny: tcp://127.0.0.1:43993, threads: 2>,\n",
      "    1: <Nanny: tcp://127.0.0.1:37469, threads: 2>,\n",
      "    2: <Nanny: tcp://127.0.0.1:37555, threads: 2>,\n",
      "    3: <Nanny: tcp://127.0.0.1:43495, threads: 2>}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a5de54b56228>\u001b[0m in \u001b[0;36mdask_convert_data\u001b[0;34m(count, nworkers, multiple)\u001b[0m\n\u001b[1;32m     57\u001b[0m         meta={\n\u001b[1;32m     58\u001b[0m             \u001b[0;34m'Binary Data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'object'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;34m'Rec Length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int64'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         }\n\u001b[1;32m     61\u001b[0m     ).compute()\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2711\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2712\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2713\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2714\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2715\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1990\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1992\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1993\u001b[0m             )\n\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             return sync(\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m             )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df = dask_convert_data(count=5000, nworkers=4, multiple=1)\n",
    "# df = dask_convert_data(count=50000, nworkers=4, multiple=2)\n",
    "df = dask_convert_data(count=500000, nworkers=4, multiple=2)\n",
    "# df = dask_convert_data(count=5000000, nworkers=4)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 worker\n",
    "Using i7-7700HQ @ 3.54-3.60 GHz\n",
    "\n",
    "- for ~35,000 records --> 5.50 secs (159% pandas)\n",
    "- for ~350,000 records --> 41.26 secs (117% pandas)\n",
    "- for ~3,500,000 records --> 6 mins 41 secs (101% pandas)\n",
    "- for ~35,000,000 records --> Not Measured (likely MemoryError)\n",
    "   - See Pandas performance for how long this took\n",
    "   - Spoiler (or not) ... it was a long time! :)\n",
    "\n",
    "Notes - Some single threaded loads initially. Then upto 27% load. Memory usage up from 8GB to 11GB.\n",
    "\n",
    "#### 2 workers\n",
    "Using i7-7700HQ @ 3.46-3.55 GHz\n",
    "\n",
    "- for ~35,000 records --> 4.4 secs (128% pandas)\n",
    "- for ~350,000 records --> 26.02 secs (74% of pandas)\n",
    "- for ~3,500,000 records --> 4 mins 11.79 secs (64% pandas)\n",
    "- for ~35,000,000 records --> Not Measured (likely MemoryError)\n",
    "\n",
    "Notes - Some single threaded loads initially. Then upto 44% load. Memory usage up from 8GB to 11GB.\n",
    "\n",
    "#### 3 workers\n",
    "Using i7-7700HQ @ 3.41-3.47 GHz\n",
    "\n",
    "- for ~35,000 records --> 3.81 secs (110% pandas)\n",
    "- for ~350,000 records --> 18.52 secs (52% of pandas)\n",
    "- for ~3,500,000 records --> 2 mins 38.70 secs (40% pandas)\n",
    "- for ~35,000,000 records --> Not Measured (likely MemoryError)\n",
    "\n",
    "Notes - Some single threaded loads initially. Then upto 62% load. Memory usage up from 8GB to 11GB.\n",
    "\n",
    "#### 4 workers\n",
    "\n",
    "Using TR1950 with 16GB RAM\n",
    "- for ~3,500,000 records --> 1 mins 29.97 secs\n",
    "- for ~7,000,000 records --> x mins x secs\n",
    "- for ~10,500,000 records --> x mins x secs\n",
    "- for ~14,000,000 records --> x mins x secs\n",
    "\n",
    "Using i7-7700HQ @ 3.40-3.45 GHz\n",
    "- for ~35,000 records --> 3.65 secs (106% pandas)\n",
    "- for ~350,000 records --> 16.34 secs (46% pandas)\n",
    "- for ~3,500,000 records --> 2 mins 19.67 secs (35% pandas)\n",
    "   - RAM used 8.8 GB\n",
    "- for ~7,000,000 records --> 4 mins 32.21 secs\n",
    "   - RAM used 12.4 GB\n",
    "- for ~10,500,000 records --> 6 mins 46.21 secs\n",
    "   - RAM used 15.6 GB\n",
    "- for ~35,000,000 records --> MemoryError\n",
    "   - RAM limited at 15.9 GB\n",
    "\n",
    "Notes - Some single threaded loads initially. Then upto 91% load. Memory usage up from 8GB to 11GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = [\n",
    "    ['Pandas', '35,000', 3.45],\n",
    "    ['Pandas', '350,000', 35.40],\n",
    "    ['Pandas', '3,500,000', 396.00],\n",
    "    \n",
    "    ['Dask 1', '35,000', 5.50],\n",
    "    ['Dask 1', '350,000', 41.26],\n",
    "    ['Dask 1', '3,500,000', 401.00],\n",
    "    \n",
    "    ['Dask 2', '35,000', 4.40],\n",
    "    ['Dask 2', '350,000', 26.02],\n",
    "    ['Dask 2', '3,500,000', 251.79],\n",
    "    \n",
    "    ['Dask 3', '35,000', 3.81],\n",
    "    ['Dask 3', '350,000', 18.52],\n",
    "    ['Dask 3', '3,500,000', 158.70],\n",
    "    \n",
    "    ['Dask 4', '35,000', 3.65],\n",
    "    ['Dask 4', '350,000', 16.34],\n",
    "    ['Dask 4', '3,500,000', 139.67],\n",
    "    ['Dask 4', '7,000,000', 272.21],\n",
    "    ['Dask 4', '10,500,000', 406.21],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results_list, columns=['Method', 'Records', 'Seconds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_all = sns.barplot(x=\"Records\", y=\"Seconds\", hue=\"Method\", data=df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns_1st = sns.barplot(x=\"Records\", y=\"Seconds\", hue=\"Method\", data=df_results[df_results.Records=='35,000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns_2nd = sns.barplot(x=\"Records\", y=\"Seconds\", hue=\"Method\", data=df_results[df_results.Records=='350,000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_all = sns_all.get_figure()\n",
    "fig_1st = sns_1st.get_figure()\n",
    "fig_2nd = sns_2nd.get_figure()\n",
    "\n",
    "fig_all.savefig('./imgs/all_Records.png')\n",
    "fig_1st.savefig('./imgs/1st_Records.png')\n",
    "fig_2nd.savefig('./imgs/2nd_Records.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
